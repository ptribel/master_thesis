{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Generative Adversarial Networks\n",
    "## Initialisation and dataset preparation\n",
    "\n",
    "First, let us import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from misceallaneous import getWavFileAsNpArray, displaySpectrogram\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us include the dataset.\n",
    "\n",
    "The dataset is made of two files: `clean/p1.wav`and `white/p1.wav` which are converted into arrays of `int32` and then split into segments of `samples_length`.\n",
    "\n",
    "The goal of the CGAN here is to predict the clean sample, when fed with the white one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_family = \"db38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate = 12000\n",
    "nperseg = 1024\n",
    "\n",
    "clean = getWavFileAsNpArray(\"../dataset_2/clean/p1.wav\")\n",
    "white = getWavFileAsNpArray(\"../dataset_2/white/p1.wav\")\n",
    "clean = np.array(clean, dtype=\"float32\")\n",
    "white = np.array(white, dtype=\"float32\")\n",
    "\n",
    "clean_dataset = []\n",
    "white_dataset = []\n",
    "\n",
    "samples_length = nperseg*2\n",
    "\n",
    "for i in range(0, clean.shape[0]-samples_length, samples_length):\n",
    "    clean_dataset.append(clean[i:i+samples_length])\n",
    "    white_dataset.append(white[i:i+samples_length])\n",
    "clean_dataset = np.array(clean_dataset)\n",
    "white_dataset = np.array(white_dataset)\n",
    "\n",
    "\n",
    "ex = clean_dataset[0]\n",
    "ca, cd = pywt.dwt(ex, wavelet_family, \"per\")\n",
    "data_shape = ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.790526\n",
      "(5329, 2048)\n"
     ]
    }
   ],
   "source": [
    "wavelet_clean_dataset = []\n",
    "wavelet_white_dataset = []\n",
    "\n",
    "for sample in clean_dataset:\n",
    "    ca, cd = pywt.dwt(sample, wavelet_family, \"per\")\n",
    "    wavelet_clean_dataset.append(np.concatenate((ca, cd)))\n",
    "for sample in white_dataset:\n",
    "    ca, cd = pywt.dwt(sample, wavelet_family, \"per\")\n",
    "    wavelet_white_dataset.append(np.concatenate((ca, cd)))\n",
    "    \n",
    "max_clean = np.max(np.abs(wavelet_clean_dataset))\n",
    "wavelet_clean_dataset = np.array(wavelet_clean_dataset)/(max_clean)\n",
    "\n",
    "max_white = np.max(np.abs(wavelet_white_dataset))\n",
    "wavelet_white_dataset = np.array(wavelet_white_dataset)/(max_white)\n",
    "\n",
    "print(np.max(wavelet_white_dataset))\n",
    "print(wavelet_white_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_audio(data, g=None, p=0, i=0):\n",
    "    audio = hear_audio(data, g, p, i)\n",
    "    D = librosa.stft(audio)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    librosa.display.specshow(S_db, x_axis='time', y_axis='mel', sr=samplerate)\n",
    "    #plt.show()\n",
    "    plt.savefig(str(p)+\".png\", format='png')\n",
    "    \n",
    "def hear_audio(data, g=None, p=0, i=0):\n",
    "    if g == None:\n",
    "        ca = data[i, :data_shape[0]]*max_clean\n",
    "        cd = data[i, data_shape[0]:]*max_clean\n",
    "    else:\n",
    "        cacd = g.predict(np.reshape(data[i], (-1, data_shape[0]*2)))[0]\n",
    "        ca = cacd[:data_shape[0]]*max_clean\n",
    "        cd = cacd[data_shape[0]:]*max_clean\n",
    "    audio = pywt.idwt(ca, cd, wavelet_family, \"per\")\n",
    "    return audio\n",
    "\n",
    "def get_distance_audio(white, clean, g, n):\n",
    "    res = 0\n",
    "    for i in range(n):\n",
    "        d = np.reshape(white[i], (-1, data_shape[0]*2))\n",
    "        data = (g.predict(d))[0]\n",
    "        res += np.sum((data-clean[i])**2)\n",
    "    return res/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Model\n",
    "The main idea of a GAN model is to create two networks who play an adversarial game:\n",
    "- A Generator, whose goal is to produce the most realistic samples possible to fool the Discriminator\n",
    "- A Discriminator, whose goal is to correctly guess if its input is a real sample from the clean dataset or an output created by the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(sizes, lr=0.0001):\n",
    "    inputs = tf.keras.Input(shape=(sizes[1]))\n",
    "    x = tf.keras.layers.Dropout(0.3)(inputs)\n",
    "    x1 = tf.keras.layers.Dense(512, activation=\"tanh\")(x)\n",
    "    x2 = tf.keras.layers.Dense(64, activation=\"tanh\")(x1)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"tanh\")(x2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"discriminator\")\n",
    "    model.summary()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The generator itself is a Convolutionnal Autoencoder.\n",
    "\n",
    "Its input size and output size are both the size of the stft array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sizes, lr=0.0001):\n",
    "    inputs = tf.keras.Input(shape=(sizes[1]))\n",
    "    x = tf.keras.layers.Dropout(0.2)(inputs)\n",
    "    x1 = tf.keras.layers.Dense(512, activation=\"tanh\")(x)\n",
    "    x2 = tf.keras.layers.Dense(128, activation=\"relu\")(x1)\n",
    "    x3 = tf.keras.layers.Dense(512, activation=\"tanh\")(x2)\n",
    "    #a = tf.keras.layers.Add()([x1, x3])\n",
    "    outputs = tf.keras.layers.Dense(sizes[1], activation=\"tanh\")(x3)\n",
    "    #outputs = tf.keras.layers.Add()([inputs, x4])\n",
    "    #outputs = tf.keras.layers.Dense(sizes[1], activation=\"tanh\")(x1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"autoencoder\")\n",
    "    model.summary()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_outputs(white, train_size, g, nperseg, clean):\n",
    "    steps = train_size//20\n",
    "    rng = np.random.default_rng()\n",
    "    g_outputs = []\n",
    "    batch = rng.choice(white, train_size)\n",
    "    for i in range(train_size):\n",
    "        if i%steps == 0:\n",
    "            print(\"=\", end='')\n",
    "        t = np.reshape(white[i, :], (-1, white.shape[1]))\n",
    "        m = g.predict(t)\n",
    "        g_outputs.append(m)\n",
    "    print()\n",
    "    g_outputs = np.reshape(np.array(g_outputs), (train_size,  white.shape[1]))\n",
    "    input_data = np.concatenate((g_outputs, clean[:train_size,]))\n",
    "    output_data = np.concatenate((np.zeros((train_size,)), np.ones((train_size,))))\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, size, g, d):\n",
    "        self.g = g\n",
    "        self.d = d\n",
    "        self.size = size\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.z = self.g.inputs\n",
    "        self.image = self.g(self.z)\n",
    "        self.valid = self.d(self.image)\n",
    "        self.combined_network = tf.keras.Model(self.z, self.valid)\n",
    "        self.compile()\n",
    "        \n",
    "    def block_discriminator(self):\n",
    "        self.d.trainable = False\n",
    "        self.g.trainable = True\n",
    "        self.build()\n",
    "        \n",
    "    def block_generator(self):\n",
    "        self.g.trainable = False\n",
    "        self.d.trainable = True\n",
    "        self.build()\n",
    "        \n",
    "    def compile(self):\n",
    "        self.combined_network.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(d, i, o, validation_split=0, batch_size=16, verbose=True):\n",
    "    history = d.fit(i, o, batch_size=batch_size, validation_split=validation_split, verbose=verbose)\n",
    "    return np.mean(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_train_size = wavelet_white_dataset.shape[0]\n",
    "generator_train_size = wavelet_white_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 2048)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,081,985\n",
      "Trainable params: 1,081,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 2048)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 2048)              1050624   \n",
      "=================================================================\n",
      "Total params: 2,231,424\n",
      "Trainable params: 2,231,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d = discriminator(wavelet_white_dataset.shape)\n",
    "g = generator(wavelet_white_dataset.shape)\n",
    "gan = GAN(wavelet_white_dataset.shape, g, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "q = 1\n",
    "e = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "=====================\n",
      "Training the discriminator\n",
      "Generator error is 28.18254663810134\n",
      "667/667 [==============================] - 3s 4ms/step - loss: 0.2688 - accuracy: 0.6936\n",
      "667/667 [==============================] - 2s 4ms/step - loss: 0.0340 - accuracy: 0.9619\n",
      "Training the generator\n",
      "1333/1333 [==============================] - 13s 10ms/step - loss: 0.0054 - accuracy: 0.9942\n",
      "124.16884840607644\n",
      "Step 1\n",
      "="
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-2135cee75570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0me\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md_accuracy\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_generator_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavelet_white_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_train_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnperseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavelet_clean_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distance_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavelet_white_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavelet_clean_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d9d7fdb1379c>\u001b[0m in \u001b[0;36mget_generator_outputs\u001b[0;34m(white, train_size, g, nperseg, clean)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mg_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[1;32m   1597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1599\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    385\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     dataset = dataset.map(\n\u001b[0m\u001b[1;32m    388\u001b[0m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1805\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m       return ParallelMapDataset(\n\u001b[0m\u001b[1;32m   1808\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4241\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4242\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4243\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3515\u001b[0m           \u001b[0mautograph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m-> 3517\u001b[0;31m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3518\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m   3791\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m   3792\u001b[0m         \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3793\u001b[0;31m         Function(\n\u001b[0m\u001b[1;32m   3794\u001b[0m             \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3795\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, python_function, name, input_signature, attributes, autograph, autograph_options, experimental_relax_shapes, capture_by_value, experimental_compile, experimental_follow_type_hints)\u001b[0m\n\u001b[1;32m   2910\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m     \u001b[0mpure_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mIMPLEMENTS_ATTRIBUTE_NAME\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m     self._function_spec = FunctionSpec.from_function_and_signature(\n\u001b[0m\u001b[1;32m   2913\u001b[0m         \u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0minput_signature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mfrom_function_and_signature\u001b[0;34m(python_function, input_signature, is_pure, experimental_follow_type_hints, experimental_compile)\u001b[0m\n\u001b[1;32m   2336\u001b[0m       \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mFunctionSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m     \"\"\"\n\u001b[0;32m-> 2338\u001b[0;31m     \u001b[0mfullargspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0;31m# Treat a wrapped partial function as a special case. For all arguments that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0;31m# were overridden with keywords in the partial:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_maybe_argspec_to_fullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_getfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;31m# so we ensure that remains the case in 3.3+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         sig = _signature_from_callable(func,\n\u001b[0m\u001b[1;32m   1124\u001b[0m                                        \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m                                        \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2292\u001b[0;31m         return _signature_from_function(sigcls, obj,\n\u001b[0m\u001b[1;32m   2293\u001b[0m                                         skip_bound_arg=skip_bound_arg)\n\u001b[1;32m   2294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeyword_only_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m         parameters.append(Parameter(name, annotation=annotation,\n\u001b[0m\u001b[1;32m   2176\u001b[0m                                     kind=_VAR_POSITIONAL))\n\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2510\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'implicit{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2512\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misidentifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2513\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is not a valid parameter name'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disc_acc = []\n",
    "gen_loss = [0]\n",
    "gan_acc = []\n",
    "while p < 200:\n",
    "    g_accuracy = 0\n",
    "    d_accuracy = 0\n",
    "    print(\"Step\", e)\n",
    "    e+=1\n",
    "    if d_accuracy < 1:\n",
    "        i, o = get_generator_outputs(wavelet_white_dataset, discriminator_train_size, gan.g, nperseg, wavelet_clean_dataset)\n",
    "    gan.block_generator()\n",
    "    err = get_distance_audio(wavelet_white_dataset, wavelet_clean_dataset, gan.g, 100)\n",
    "    print(\"Training the discriminator\")\n",
    "    print(\"Generator error is\", err)\n",
    "    while d_accuracy < 0.9:\n",
    "        d_accuracy = train_on_batch(gan.d, i, o, verbose=True)\n",
    "        disc_acc.append(d_accuracy)\n",
    "        gan_acc.append(0)\n",
    "        gen_loss.append(err)\n",
    "    gan.block_discriminator()\n",
    "    print(\"Training the generator\")\n",
    "    while g_accuracy < 0.95:\n",
    "        g_accuracy = train_on_batch(gan.combined_network, wavelet_white_dataset[:generator_train_size], np.ones(generator_train_size), batch_size=4, verbose=True)\n",
    "        gan_acc.append(g_accuracy)\n",
    "        disc_acc.append(0)\n",
    "        err = get_distance_audio(wavelet_white_dataset, wavelet_clean_dataset, gan.g, 100)\n",
    "        print(err)\n",
    "        gen_loss.append(err)\n",
    "        display_audio(wavelet_white_dataset, gan.g, p)\n",
    "        p+=1\n",
    "    #print(evaluate_generator(gan.g, white_dataset, clean_dataset))\n",
    "plt.plot(disc_acc)\n",
    "plt.plot(gan_acc)\n",
    "plt.show()\n",
    "plt.plot(gen_loss[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#gan.combined_network.save('save4/gan_with_add')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

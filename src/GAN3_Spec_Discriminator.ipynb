{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Generative Adversarial Networks Discriminator\n",
    "## Initialisation and dataset preparation\n",
    "\n",
    "First, let us import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram, stft, istft\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from misceallaneous import getWavFileAsNpArray, displaySpectrogram\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us include the dataset.\n",
    "\n",
    "The dataset is made of two files: `clean/p1.wav`and `white/p1.wav` which are converted into arrays of `int32` and then split into segments of `samples_length`.\n",
    "\n",
    "The goal of the CGAN here is to predict the clean sample, when fed with the white one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate = 12000\n",
    "nperseg = 1024\n",
    "\n",
    "clean = getWavFileAsNpArray(\"../dataset_2/clean/p1.wav\")\n",
    "white = getWavFileAsNpArray(\"../dataset_2/white/p1.wav\")\n",
    "clean = np.array(clean, dtype=\"float32\")\n",
    "white = np.array(white, dtype=\"float32\")\n",
    "\n",
    "clean_dataset = []\n",
    "white_dataset = []\n",
    "\n",
    "samples_length = nperseg*10\n",
    "\n",
    "for i in range(0, clean.shape[0]-samples_length, samples_length):\n",
    "    clean_dataset.append(clean[i:i+samples_length])\n",
    "    white_dataset.append(white[i:i+samples_length])\n",
    "clean_dataset = np.array(clean_dataset)\n",
    "white_dataset = np.array(white_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_clean_dataset = []\n",
    "spectrogram_white_dataset = []\n",
    "\n",
    "for sample in clean_dataset:\n",
    "    spectrogram_clean_dataset.append(librosa.amplitude_to_db(np.abs(librosa.stft(sample))))\n",
    "for sample in white_dataset:\n",
    "    spectrogram_white_dataset.append(librosa.amplitude_to_db(np.abs(librosa.stft(sample))))\n",
    "\n",
    "spectrogram_clean_dataset = np.array(spectrogram_clean_dataset)\n",
    "spectrogram_white_dataset = np.array(spectrogram_white_dataset)\n",
    "\n",
    "max_clean = np.max(spectrogram_clean_dataset)\n",
    "min_clean = np.min(spectrogram_clean_dataset)\n",
    "spectrogram_clean_dataset = (spectrogram_clean_dataset-min_clean)/max_clean\n",
    "\n",
    "max_white = np.max(spectrogram_white_dataset)\n",
    "min_white = np.min(spectrogram_white_dataset)\n",
    "spectrogram_white_dataset = (spectrogram_white_dataset-min_white)/max_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = (spectrogram_white_dataset.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1025, 21)\n",
      "Dataset shape: (1065, 1025, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data_shape)\n",
    "print(\"Dataset shape:\", spectrogram_white_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7244133 0.8820283\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.abs(spectrogram_clean_dataset)), np.max(np.abs(spectrogram_white_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator here uses a layer to process the Short-Time Fourier Transform (https://en.wikipedia.org/wiki/Short-time_Fourier_transform) before reducing the problem dimension to one single boolean prediction layer.\n",
    "\n",
    "Interestingly, adding a Dropout layer on the input seems to prevent the generator to adapt itself to the little flaws of detection (which then only produces noise unrecognized by the discriminator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1025, 21)]        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1025, 21)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1025, 10)          850       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 512, 10)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 512, 10)           410       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 256, 10)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 256, 10)           410       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 128, 10)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128, 1)            11        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,810\n",
      "Trainable params: 1,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def discriminator(input_shape):\n",
    "    inputs = tf.keras.Input(shape=(input_shape[1], input_shape[2]))\n",
    "    x = tf.keras.layers.Dropout(0.3)(inputs)\n",
    "    x1 = tf.keras.layers.Convolution1D(10, kernel_size=(4), activation=\"tanh\", padding=\"same\")(x)\n",
    "    x2 = tf.keras.layers.MaxPooling1D()(x1)\n",
    "    x3 = tf.keras.layers.Convolution1D(10, kernel_size=(4), activation=\"tanh\", padding=\"same\")(x2)\n",
    "    x4 = tf.keras.layers.MaxPooling1D()(x3)\n",
    "    x5 = tf.keras.layers.Convolution1D(10, kernel_size=(4), activation=\"tanh\", padding=\"same\")(x4)\n",
    "    x6 = tf.keras.layers.MaxPooling1D()(x5)\n",
    "    x7 = tf.keras.layers.Dense(1, activation=\"tanh\")(x6)\n",
    "    x8 = tf.keras.layers.Flatten()(x7)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x8)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"discriminator\")\n",
    "    model.summary()\n",
    "    model.compile(optimizer= 'adam', loss='mse', metrics=['accuracy'])\n",
    "    return model\n",
    "d = discriminator(spectrogram_white_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(d, i, o, validation_split=0, batch_size=16, verbose=True):\n",
    "    history = d.fit(i, o, batch_size=batch_size, validation_split=validation_split, verbose=verbose)\n",
    "    return np.mean(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_train_size = (spectrogram_white_dataset.shape[0])\n",
    "generator_train_size = spectrogram_white_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 2s 13ms/step - loss: 0.1345 - accuracy: 0.8309\n"
     ]
    }
   ],
   "source": [
    "d_accuracy = 0\n",
    "while d_accuracy < 0.9:\n",
    "    d_accuracy = train_on_batch(d, np.concatenate((spectrogram_white_dataset, spectrogram_clean_dataset)), np.concatenate((np.zeros(spectrogram_white_dataset.shape[0]), np.ones(spectrogram_clean_dataset.shape[0]))), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
